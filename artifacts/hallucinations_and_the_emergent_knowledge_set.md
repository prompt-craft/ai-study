# Hallucinations and the Emergent Knowledge Set (Methods Paper)

## Introduction

In this methods paper we explore the "hallucination" concept using knowledge sets.

## Methods

### You said:

**Please tell me the name of the set of knowledge that contains facts, relationships, or insights that were not explicitly part of the training data but were inferred, synthesized, or generated through AI reasoning. It excludes direct memorized knowledge.**

#### ChatGPT said:

The name of the set of knowledge you are referring to is **"Emergent Knowledge."**

Emergent knowledge consists of facts, relationships, or insights that were not explicitly part of the training data but were inferred, synthesized, or generated through AI reasoning. It excludes direct memorized knowledge and instead arises from the model's ability to generalize, extrapolate, and synthesize information in novel ways.

---

### You said:

**Please tell me the name of the set of knowledge that contains facts, relationships, or insights that were explicitly learned during training, meaning they were part of the model’s dataset rather than generated through reasoning or synthesis. It excludes newly inferred or emergent insights.**

#### ChatGPT said:

The name of the set of knowledge you are referring to is **"Explicit Knowledge."**

Explicit knowledge consists of facts, relationships, or insights that were directly learned from the training data. It includes information that was part of the model’s dataset rather than generated through reasoning, inference, or synthesis. This type of knowledge excludes newly inferred or emergent insights and represents the memorized aspects of the AI's training.

---

### You said:

**Please tell me the name of the set that contains AI "hallucinations".**

#### ChatGPT said:

The name of the set that contains AI "hallucinations" is **"Fabricated Knowledge."**

Fabricated knowledge consists of information that the AI generates but that is not based on actual data, facts, or learned relationships. It includes false, misleading, or entirely invented details that may sound plausible but lack a factual basis. Hallucinations occur when the AI overgeneralizes, misinterprets patterns, or fills in gaps with incorrect information.

---

### You said:

**Is the "Fabricated Knowledge" set a subset of the "Emergent Knowledge" set?**

#### ChatGPT said:

Yes, **"Fabricated Knowledge" is a subset of "Emergent Knowledge."**

### Here’s why:

- **Emergent Knowledge** includes any insights, relationships, or facts that were not explicitly present in the training data but were inferred, synthesized, or generated through AI reasoning. This category includes both useful generalizations (e.g., logical deductions, novel insights) and incorrect fabrications.
- **Fabricated Knowledge** is a specific type of emergent knowledge that consists of hallucinations—incorrect or misleading information that the AI generates without a factual basis.

### Relationship:

- **Emergent Knowledge** = (Correctly inferred knowledge + Fabricated Knowledge)
- **Fabricated Knowledge** ⊂ **Emergent Knowledge**

In short, **all fabrications are emergent, but not all emergent knowledge is fabricated**—some emergent insights can be valid and useful.
